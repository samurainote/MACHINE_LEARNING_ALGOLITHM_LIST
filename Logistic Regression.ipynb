{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "minmaxscaler used for normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as  pd\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
       " 'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': '/usr/local/lib/python3.6/dist-packages/sklearn/datasets/data/iris.csv',\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10')}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n"
     ]
    }
   ],
   "source": [
    "X = iris.data[:, :]\n",
    "y = (iris.target != 0) * 1\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='b', label='0')\n",
    "plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='r', label='1')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, verbose=False):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    def __loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        # weights initialization\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - y)) / y.size\n",
    "            self.theta -= self.lr * gradient\n",
    "            \n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            loss = self.__loss(h, y)\n",
    "                \n",
    "            if(self.verbose ==True and i % 10000 == 0):\n",
    "                print(f'loss: {loss} \\t')\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "    \n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.predict_prob(X).round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=1e20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.84 ms, sys: 0 ns, total: 3.84 ms\n",
      "Wall time: 3.95 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1e+20, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(X)\n",
    "(preds == y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.90025946]),\n",
       " array([[-1.50762868, -4.931823  ,  7.81011211,  3.79677487]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept_, model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X, y = load_iris(return_X_y=True)\n",
    "clf = LogisticRegression().fit(X, y)\n",
    "a=[[2.3,3,5,4.5],[2.4,3.2,3.1,4.3]]\n",
    "#a=[2.3,3,5,4.5]\n",
    "#a.array.reshape(-1,1)\n",
    "\n",
    "clf.predict(a)\n",
    "#clf.predict_proba(X[:2, :]) \n",
    "#clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as  pd\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :]\n",
    "y = (iris.target != 0) * 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4)\n",
      "(100,)\n",
      "(50, 4)\n",
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "X_train,Y_train,X_test,Y_test=train_test_split(X,y,test_size=(0.33))\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler()\n",
    "train_minmax = minmax.fit_transform(X_train)\n",
    "test_minmax = minmax.fit_transform(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#fitting model\n",
    "clf = LogisticRegression().fit(X, y)\n",
    "clf_min_max=LogisticRegression().fit(train_minmax,X_test)\n",
    "print(clf.score(Y_train,Y_test))\n",
    "print(clf_min_max.score(Y_train,Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
